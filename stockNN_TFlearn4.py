
"""
=====================================================================================================
this program is a DNN stock models' producer
it reads hyper parameters from a csv file "HyperParamSearchPlan.csv" and train models accordingly,
then save those models including datapreprocess,hyperparameters to disk,  evaluate each model with pre-defined training set and test set,
record their metrics,such as AUC, accuracy,F1 score, update the DNN_training_results.csv file
========================================================================================================
"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

print(__doc__)

import time
import tensorflow as tf

# the following packages  are part of the project
from utility import log, plotFeatures, duration, datasetSplit, datasetShuffle
from fetchData import FetchData
from dnnModel import DnnModel
from preprocess import DataPreprocess
from hyperParam import HyperParam, supportedSkip, selectedAvgline
from modelStore import ModelStore



hyperParamSetFile = "HyperParamSearchPlan.csv"          # file is generated by genParamCsv.py and reviewed manually


# from sklearn.externals import joblib

def main():
    """
      this is a model producer, it reads hyper parameters from hyperParamSetFile , setting hyper params accordingly,
      build and train models.

      :return:
      its output are
      1. a csv file that track all the models , including the model's hyper parameters, file locations in disk
      2. a series of (model files + accompanied data preprocess instance file & hyper parameter file)
    """

    # show the overall profit or loss rate when strictly follow the rule of buying and selling stock
    # for each selectedAvgLine
    # load_csv_calc_profit(trainfilename,start_rowid=TrainDataStart,end_rowid=TrainDataStop,
    #                      fromDate='2010/12/31',
    #                      toDate='2016/12/29',
    #                      target_column=-1,
    #                      selectedAvgline=selectedAvgline)
    #
    # load_csv_calc_profit(testfilename, start_rowid=TestDataStart, end_rowid=TestDataStop,
    #                      fromDate='2016/12/30',
    #                      toDate='2017/09/19',
    #                      target_column=-1,
    #                      selectedAvgline=selectedAvgline)
    # return

    runStartTime = time.time()  # start time in ms.
    st = time.ctime()  # time in date/time format

    # instantiate a HyperParam class to read  hyperparameter search plan into a list
    # do sanity check to make sure all input parameters are valid before going further
    try:
        hpIns = HyperParam(hyperParamSetFile)
        hpIns.sanityCheck()
    except ValueError as e:
        log("ValueError Exception:")
        log(e.message)
        return
    except KeyError as e:
        log("KeyError Exception happened")
        log(e.message)
        return
    else:
        log("\n sanity check on search plan file(%s) PASSED " % hyperParamSetFile)

    for seqid in range(0, hpIns.rows.__len__()):
        loopstartTime = time.time()  # start time in ms.
        try:
            preProcessChanged, trainDataChanged, testDataChanged, hpDict = hpIns.readRow(rowId=seqid)
        except ValueError as e:
            log("WARNING: exception captured")
            log(e.message)
            log("\nSkip seqid=%d  and continue to next iteration, please double check your settings in %s"
                % (seqid, hpIns.filename))
            continue

        if hpDict['Skip'] == supportedSkip[2] or hpDict['Skip'] == supportedSkip[3]:  # this row is comment out, not run
            continue

        # create an instance of FetchData class to load datasets.
        fd = FetchData()
        if trainDataChanged:
            log('\n%d ---> loading training data from:%s to:%s in progress ... time:%s'
                % (seqid, hpDict["TFromDate"], hpDict["TToDate"],  time.ctime()))

            try:
                training_set = fd.loadData(hpDict["TFromDate"], hpDict["TToDate"])
            except ValueError as e:
                print('=' * 30 + "ValueError Exception happened:" + '=' * 30)
                print(e)
                print('=' * 30 + "end of print ValueError exception" + '=' * 30)
                log("ValueError occurred in loading training data, skip this iteration")
                continue

            # plot original data to review
            #plotFeatures(training_set.data,training_set.featurenames,[0,1,2],
            #             hpDict["TFromDate"] + '/' + hpDict["TToDate"],
            #             savePlotToDisk=True,scatterAdjust=False)
        else:
            log("Training data unchanged from last row, skip reloading training data from files,reusing the last one")
        if testDataChanged:
            log('\nloading test data from:%s to:%s in progress ... time:%s'
                % (hpDict["TestFromD"], hpDict["TestToD"], time.ctime()))

            try:
                test_set = fd.loadData(hpDict["TestFromD"], hpDict["TestToD"])
            except ValueError as e:
                print('=' * 30 + "ValueError Exception happened:" + '=' * 30)
                print(e)
                print('=' * 30 + "end of print ValueError exception" + '=' * 30)
                log("ValueError occurred in loading test data, skip this iteration")
                continue
            # plot original test data to review
            #plotFeatures(test_set.data,test_set.featurenames,[0, 1, 2],
            #    hpDict["TestFromD"]+ '/' + hpDict["TestToD"],savePlotToDisk=True,scatterAdjust=False)
        else:
            log("Test data unchanged from last row, skip reloading test data from files,reusing the last one")

        dp = DataPreprocess(hpDict['Preprocessor'])
        if not preProcessChanged:
            if trainDataChanged:
                log('\nSeqno %d : preProcess is required to be reapply,since we need to apply a '
                    'preprocessor to a different training data from last run' % seqid)
                dp.fit(training_set)
                X = dp.transform(training_set)
                y = training_set.target
                X_test = dp.transform(test_set)  # use the same scaler to transform test_set.data
                y_test = test_set.target
                # for debugging purpose, plot any  feature ids for manually checking their values after transform
                #plotFeatures(X, training_set.featurenames, [0,1,2,43,112],
                #             hpDict['Preprocessor']+hpDict["TFromDate"] + '/' + hpDict["TToDate"],
                #             savePlotToDisk=True,scatterAdjust=False)
            elif testDataChanged:
                log('\nSeqno %d : preProcess is required to be reapply on test data only,since we need to apply a '
                    'preprocessor to a different test data from last run' % seqid)
                X_test = dp.transform(test_set)  # use the same scaler to transform test_set.data
                y_test = test_set.target
            else:
                log("\nSeqno %d : Skip preprocess since this is the same training, "
                    "test data and preprocessor as last time,"
                    "X and X_test have been preprocessed last time" % seqid)
        else:
            log('\nSeqno %d : preProcess is required to be reapply,since we need to apply a '
                ' different preprocessor from last run' % seqid)
            dp.fit(training_set)
            X = dp.transform(training_set)
            y = training_set.target
            X_test = dp.transform(test_set)  # use the same scaler to transform test_set.data
            y_test = test_set.target
            # for debugging purpose, plot any  feature ids for manually checking their values after transform
            #plotFeatures(X, training_set.featurenames, [0,1,2,43,112],
            #            hpDict['Preprocessor'] + hpDict["TFromDate"] + '/' + hpDict["TToDate"],
            #           savePlotToDisk=True,scatterAdjust=False)

        # at first, shuffle the training data,
        # then split training data into a training_set and a training_dev set, both have same distribution,
        # by comparing the performance difference of the training set and training_dev set, we know how good the model
        # is in capturing the training set data's core attributes
        # it's a good way to judge if the model is overfit, if training auc  >> training_dev auc.

        if trainDataChanged or preProcessChanged:
            X,y = datasetShuffle(X,y)
            #  print ('skip datashuffle, this time =========================> compare with shuffle')
            X_train,y_train,X_traindev,y_traindev = datasetSplit(X,y,splitRate=0.01)

        # try pca
        # X_test = pca.transform(X_test)
        log('\nbuilding model in progress ... time:%s' % (time.ctime()))
        tf.reset_default_graph()
        try:
            mymodel = DnnModel(hpDict)
            mymodel.train(X_train, y_train, X_test, y_test)

            # save the whole model including its data preprocess method and datascaler in disk,
            # remember its location in a file, so that it can be loaded later
            modelStore = ModelStore()
            modelStore.save(hpDict, mymodel, dp)

            # evaluate the model with training/test set,save result to DNN_Training_result.
            modelStore.evaluate(seqid,mymodel,dp,X_train,y_train,X_traindev,y_traindev,"shuffleTrainDev")
            modelStore.evaluate(seqid, mymodel, dp, X_train, y_train, X_test, y_test,"TestSet")

            # calculate the duration of this loop, update record
            loopElapsedTime = duration(loopstartTime)
            log("\nthe time of building/training/evaluating the model is %s" % loopElapsedTime)
            modelStore.writeResult(seqid, hpDict, mymodel, st, time.ctime(), loopElapsedTime)

            # reduce a reference to instance of DNNModel since its task has come to an end.
            # let system to gabage collect it
            del mymodel
            del modelStore
        except ValueError as ve:
            log("Value Exception happened,bypass this iteration")
            log(ve.message)
            continue
        except IOError as ie:
            log("IOError exception occured. bypass this iteration")
            log(ie.message)
            continue
        except Exception as e:
            log("Exception happened.bypass this iteration")
            log(e.message)
            continue
        # else:   #only predict and evaluate ,skip training process
        #     #initiate a empty model, load the weight from the the saved model to reevaluate,double check the model's load function
        #     log('\nloading test data from file %s from:%s,to:%s in progress ... time:%s'
        #         % (hpDict["Test"], hpDict["TestFromD"], hpDict["TestToD"], time.ctime()))
        #     fd = FetchData(hpDict["Test"], hpDict["TestFromD"], hpDict["TestToD"], TestDataStart, TestDataStop)
        #     try:
        #         test_set = fd.loadData()
        #     except ValueError as e:
        #         print('=' * 30 + "ValueError Exception happened:" + '=' * 30)
        #         print(e)
        #         print('=' * 30 + "end of print ValueError exception" + '=' * 30)
        #         log("ValueError occurred in loading test data, skip this iteration")
        #         continue
        #         # plot original test data to review
        #         # plotFeatures(test_set.data,test_set.featurenames,[1],"Orig17Jan-17Septest",savePlotToDisk=True,scatterAdjust=False)
        #
        #
        #
        #
        #     tf.reset_default_graph()
        #     y_test = test_set.target
        #     loadmymodel = DnnModel(hpDict,runId+'load'+hpDict['Seqno'])  # use this runId + seqno combination as <runid> to distinguish multiple load models from original model, don't retrain the model
        #     try:
        #         loadmymodel.loadModel(hpDict,runId,dataPreprocessDumpfile)         # load its preprocess and datascaler
        #         X_test= loadmymodel.dpp.transform(test_set)  # use the same scaler to transform test_set.data
        #     except IOError as ve:  #can't find the trained model from disk
        #         log(ve.message)
        #         log("\nWARNING:  Skip this evaluation process ... ")
        #         continue
        #     except Exception:    # any other exceptions, just skip this evaluation, not a big deal
        #         log("\nWARNING:  Exception occurred, skip this evaluation... ")
        #         continue
        #     else:
        #         loadmymodel.evaluateTestSet(hpDict,X_test,y_test) # output the ROC to disk with <runid>load folder name
        #         log('\nload trained model & reevaluate completed! ')
        #     del loadmymodel  # delete instance of DNNModel since its task has come to an end. let system to gc it
        #     del test_set, X_test, y_test

    wholetime = duration(runStartTime)
    log("\nthe WHOLE ELAPSED time of loading data and training all the models is %s"
        % wholetime)


      # keyp = raw_input("\nPlease input a row# to predict: (-1 to quit)")
      # while (keyp != '-1'):
      #     rowid=int(keyp)
      #     tmp=X_test[rowid,:]
      #     #tmp=tmp.reshape(tmp.shape[0],1)
      #     pred= model.predict(tmp)
      #     print ("predicted y label = %d, true y label is %d" %(np.argmax(pred, axis=1),y_test[rowid,:] ))
      #
      #     keyp = raw_input("\nPlease input a row# to predict: (-1 to quit)")
      # log("End of program")



if __name__ == "__main__":
    main()
